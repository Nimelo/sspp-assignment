\chapter{Introduction} \label{chp:introduction}
	\begin{comment}
		Introduce the following subordinate sections. This section identifies the issuing organization and the
		details of issuance. It includes required approvals and status (DRAFT/FINAL) of the document. It is
		here that the scope is described and references identified.
	\end{comment}

\section{Document identifier} \label{s:introduction:document-identifier}
	\begin{comment}
		Uniquely identify a version of the document by including information such as the date of issue, the
		issuing organization, the author(s), the approval signatures (possibly electronic), and the status/version
		(e.g., draft, reviewed, corrected, or final). Identifying information may also include the reviewers and
		pertinent managers. This information is commonly put on an early page in the document, such as the
		cover page or the pages immediately following it. Some organizations put this information at the end
		of the document. This information may also be kept in a place other than in the text of the document
		(e.g., in the configuration management system or in the header or footer of the document).
	\end{comment}
	\textsc{Document version:} 1.0 \\
	\textsc{Date of issue:} \today \\
	\textsc{Issuing organization:} \textit{Cranfield University} \\
	\textsc{Authors:} \textit{Mateusz GASIOR} \\
	\textsc{Status:} Final
\section{Scope} \label{s:introduction:scope}
	\begin{comment}
		Identify the test items (software or system) that are the object of testing, e.g., specific attributes of the
		software, the installation instructions, the user instructions, interfacing hardware, database conversion
		software that is not a part of the operational system) including their version/revision level. Also
		identify any procedures for their transfer from other environments to the test environment.
		Supply references to the test item documentation relevant to an individual level of test, if it exists, such
		as follows:
		⎯ Requirements
		⎯ Design
		⎯ User’s guide
		⎯ Operations guide
		⎯ Installation guide
		Reference any Anomaly Reports relating to the test items.
		Identify any items that are to be specifically excluded from testing.
	\end{comment}
	Performance tests considered in this document relate to performance and comparison of different implementation of matrix--vector dot product using interfaces such as: \gls{openmp} and \gls{cuda} for \gls{CRS} and \gls{ELL} matrix storage formats. 
	
	Results are derived from the multiple test cases which are the same for the all tested sparse matrices. Performance test checks the overall implementation across different input cases. For each supported matrix storage formats we can obtain different result. It can be assumed that one of the most influential factor is number of non-zeros entries, although properties and constraints of the storage formats also have a really big impact.
	
	In case of test suits and test cases structures it is assumed that for each sparse matrix a new test suit is created which contains a sub-set of all test cases, which checks the performance of routines.
	
	Summarizing performance test don't check if the requirements in \gls{SRS} are fulfilled from the correctness and cohesion point of view. Those tests checks how good the implemented solution is good in practice among different type of data.
	
\section{References} \label{s:introduction:references}
	\begin{comment}
		List all of the applicable reference documents. The references are separated into “external” references
		that are imposed external to the project and “internal” references that are imposed from within to the
		project. This may also be at the end of the document.
	\end{comment}
	\begin{enumerate}
		\item \emph{Mateusz Gasior, \gls{SRS}, Supercomputer Simulation Tool}. Cranfield University, 2016. 
	\end{enumerate}